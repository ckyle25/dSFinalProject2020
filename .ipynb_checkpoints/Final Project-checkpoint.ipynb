{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accident Predictions in Utah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "Kyle Cornwall\n",
    "<br>\n",
    "Shushanna Mkrtychyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Motivation\n",
    "   \n",
    "Accidents happen all the time. On average, there are over 5,891,000 vehicle crashes each year. Approximately 21% of these crashes - nearly 1,235,000 - are weather-related. Weather-related crashes are defined as those crashes that occur in adverse weather.<br/><br/>\n",
    "          According to utah.heath.gov, accidents occur in Utah every 10 minutes, a person is injured in a crash every 23 minutes, and a person dies in a crash every 36 hours. There are many reasons what cause crashes in Utah; speed, failure to keep in the proper lane, distracted driving, etc. However, we believe that bad weather and temperature is often a major contributing factor that people get into car crashes. Weather acts through visibility impairments, high winds, and temperature extremes affect drivers capabilities, vehicle performance. These impacts can increase crash risk and severity. <br/><br/>\n",
    "          Being Utah residents we especially care about our friends and family’s safety. Utah has the most unpredictable weather conditions. People driving in poor conditions need to be extra careful, whether its snow, rain or wind. It is the drivers’ duty to adapt to road conditions, and avoid crashes. \n",
    "That motivated us to construct a model that can predict the number of accidents in Salt Lake County based on conducted information on Utah accidents and weather data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objectives\n",
    "\n",
    "\n",
    "  1. Can we predict the number car accidents that occur on any given day?          \n",
    "  2. How much of an impact does snow/rainfall, time of the day, holidays or season, have on crashes on Utah roads? \n",
    "  3. What values are important to use or consider, to predict the number of accidents?\n",
    "  4. Can we predict accident severity based on outside influencing factors (distracted driving, DUI, driver age class, etc...)\n",
    "  5. Can collected data be used and visualized properly to provide accurate and truthful information?  \n",
    "\n",
    "We are sampling the data with different approaches to learn the differences between weather, time, temperature, seasons that cause car accidents. We would like to know if data science can help us to accomplish our goals and understand what factors cause accidents. Does snow or wind really have a significant outcomes on a number of crashes in Utah?<br/><br/>\n",
    "Our goal is to predict if weather conditions have a major impact on crashes in Utah. The benefits that anyone can receive from this project analysis can be examined and used while driving in different weather conditions.  Everyone knows that more accidents occur during bad weather conditions. We can visualize the data and express it in charts, graphs, etc. Additionally, understanding the likely severity of an accident prior to arriving can help first responders better prepare for the scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import scipy as sc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import tree, svm, metrics, linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.metrics as metrics\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "**Accident Data:** https://gis.utah.gov/developer/applications/vehicle-collision/ - Data is provided at an accident grain in the form of a CSV download and lists the time, location, and other factors involved with the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidentPd = pd.read_csv(\"Data/DDACTS_download.csv\")\n",
    "accidentPd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather Data:** This with be gathered from the World Weather Online API via one request with an export to a CSV file. The below code snippet demonstrates this using a third party package. The CSV will then be imported to a pandas dataframe. (Documentation: https://www.worldweatheronline.com/developer/api/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wwo_hist import retrieve_hist_data\n",
    "# frequency = 24\n",
    "# start_date = '01-JAN-2016'\n",
    "# end_date = '31-DEC-2019'\n",
    "# api_key = 'a6cd8966ce0743ef986210109202202'\n",
    "# location_list = ['salt+lake+city']\n",
    "# hist_weather_data = retrieve_hist_data(api_key,\n",
    "#                                 location_list,\n",
    "#                                 start_date,\n",
    "#                                 end_date,\n",
    "#                                 frequency,\n",
    "#                                 location_label = False,\n",
    "#                                 export_csv = True,\n",
    "#                                 store_df = True)\n",
    "weatherPd = pd.read_csv(\"Data/salt+lake+city.csv\")\n",
    "weatherPd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "There are a few missing values throughout the accident data. Particularly in the city, lattitude, longitude, milepoint, and route columns. The county column seems fairly populated though and should at least allow us to narrow the missing columns we are interested in. We can then potentially include a portion of them based off of the other column information.<br/>\n",
    "\n",
    "We have filtered down to a year of data in the weather dataset for Salt Lake County so as to be sure the match time horizons between both sets. The accident data set has also been reduced to only reflect Salt Lake County. Additionally, the classification models we are going to use require the predictive features to be binary numerical flags. Consequently, we are transforming the Y/N flags to be 1/0. We will then try to predict accident severity based off of a variety of these featuers<br/>\n",
    "\n",
    "To combine the datasets we needed group the accident data by day and append the resultant counts to the 2019 weather set. This should allow us to run regression analyses to attempt to predict the number of accidents per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather2019 = weatherPd[(weatherPd['date_time'] > '2018-12-31') & (weatherPd['date_time'] < '2020-01-01')]\n",
    "saltLakeAccidents = accidentPd.loc[accidentPd[\"COUNTY_NAME\"] == \"SALT LAKE\"]\n",
    "saltLakeAccidents[\"DUI\"] = saltLakeAccidents.DUI.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"DISTRACTED_DRIVING\"] = saltLakeAccidents.DISTRACTED_DRIVING.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"WORK_ZONE_RELATED\"] = saltLakeAccidents.WORK_ZONE_RELATED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"PEDESTRIAN_INVOLVED\"] = saltLakeAccidents.PEDESTRIAN_INVOLVED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"BICYCLIST_INVOLVED\"] = saltLakeAccidents.BICYCLIST_INVOLVED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"MOTORCYCLE_INVOLVED\"] = saltLakeAccidents.MOTORCYCLE_INVOLVED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"IMPROPER_RESTRAINT\"] = saltLakeAccidents.IMPROPER_RESTRAINT.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"UNRESTRAINED\"] = saltLakeAccidents.UNRESTRAINED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"INTERSECTION_RELATED\"] = saltLakeAccidents.INTERSECTION_RELATED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"WILD_ANIMAL_RELATED\"] = saltLakeAccidents.WILD_ANIMAL_RELATED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"DOMESTIC_ANIMAL_RELATED\"] = saltLakeAccidents.DOMESTIC_ANIMAL_RELATED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"OVERTURN_ROLLOVER\"] = saltLakeAccidents.OVERTURN_ROLLOVER.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"COMMERCIAL_MOTOR_VEH_INVOLVED\"] = saltLakeAccidents.COMMERCIAL_MOTOR_VEH_INVOLVED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"TEENAGE_DRIVER_INVOLVED\"] = saltLakeAccidents.TEENAGE_DRIVER_INVOLVED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"OLDER_DRIVER_INVOLVED\"] = saltLakeAccidents.OLDER_DRIVER_INVOLVED.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"NIGHT_DARK_CONDITION\"] = saltLakeAccidents.NIGHT_DARK_CONDITION.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"SINGLE_VEHICLE\"] = saltLakeAccidents.SINGLE_VEHICLE.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"DROWSY_DRIVING\"] = saltLakeAccidents.DROWSY_DRIVING.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"ROADWAY_DEPARTURE\"] = saltLakeAccidents.ROADWAY_DEPARTURE.eq('Y').mul(1)\n",
    "saltLakeAccidents[\"MINOR_ACCIDENT\"] = saltLakeAccidents[\"CRASH_SEVERITY_ID\"] <= 2\n",
    "saltLakeAccidents[\"MINOR_ACCIDENT\"] = saltLakeAccidents[\"MINOR_ACCIDENT\"].astype(int)\n",
    "saltLakeMajorAccidents = saltLakeAccidents.loc[saltLakeAccidents[\"MINOR_ACCIDENT\"] < 1]\n",
    "slAccidentsDaily = pd.DataFrame({'accidentCount': saltLakeAccidents.groupby(pd.to_datetime(saltLakeAccidents[\"CRASH_DATETIME\"]).dt.date).size()})\n",
    "weather2019.insert(2,'date',pd.to_datetime(weather2019['date_time']).dt.date,True)\n",
    "weather2019.drop_duplicates(subset =\"date\", keep = 'first', inplace = True)\n",
    "mergedData = pd.merge(weather2019, slAccidentsDaily, left_on=\"date\", right_on=\"CRASH_DATETIME\", how='left')\n",
    "mergedDataClean = mergedData.drop(['uvIndex', 'uvIndex.1'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "**2019 Weather Trending**<br/>\n",
    "Looking at the below charts we can see that there are spikes in precipitation and snow at the year end. This is to be expected due to weather seasonality and the temperature trends. Unsurprisingly, the temperature seams to be much cooler at the beginning and ends of the year likely resulting in more of the precipitation becomeing snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(pd.to_datetime(weather2019['date_time']).dt.date,weather2019[\"precipMM\"],color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Precipitiation in MM')\n",
    "plt.title('Precipitation Amounts')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(pd.to_datetime(weather2019['date_time']).dt.date,weather2019[\"totalSnow_cm\"],color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Snow in CM')\n",
    "plt.title('Snow Amounts')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(pd.to_datetime(weather2019['date_time']).dt.date,weather2019[\"tempC\"],color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature')\n",
    "plt.title('Daily Temperature Trends')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2019 Accident Trending**<br/>\n",
    "The number of daily accidents is quite variable throughout the year. However, there do seem to be spikes in the data at year end suggesting a potential correlation to the snow and temperature trending data observed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(slAccidentsDaily,color='blue')\n",
    "plt.xlabel('Accident Date')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.title('2019 Automobile Accident Trends')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Number of Daily Accidents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter Matrix and Heat Map Visualization**<br/>\n",
    "By looking at Heat Map we can see the correlation between the variables. The variables that are most correlated with accidnets are Total Snow, Cloud Cover, Wind Dir. and PrecipMM (which is strongly correlated with Total Snow).\n",
    "In general we would think that Visibily would have a major/significant impact on accidents, but from the correlaton map we observe that it's not very correlated. Can we conclude or observe that people drive more careful when visibility is poor? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(mergedDataClean[['tempC', 'visibility', 'totalSnow_cm', 'sunHour', 'DewPointC', 'WindGustKmph', 'cloudcover', 'precipMM','accidentCount']], figsize=(15,15))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDataClean.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "heatmap = plt.pcolor(mergedDataClean.corr(), cmap=plt.cm.Blues, vmin=-1, vmax=1)\n",
    "plt.colorbar(heatmap)\n",
    "plt.xticks(np.arange(0, 19)+0.5, mergedDataClean.corr().columns, fontsize=14, rotation=90)\n",
    "plt.yticks(np.arange(0, 19)+0.5, mergedDataClean.corr().columns, fontsize=14)\n",
    "plt.title('Accident Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowOls = sm.ols(formula=\"accidentCount ~ totalSnow_cm\", data=mergedDataClean).fit()\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(x=mergedDataClean.totalSnow_cm,y=mergedDataClean.accidentCount,c='k',marker='*',label='Number of Accidents')\n",
    "plt.plot(mergedDataClean.totalSnow_cm,snowOls.predict(),'k',color='red',linewidth=3)\n",
    "\n",
    "plt.xlabel('Amount of Snow (CM)')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➡ \n",
    "Creating our First Linear Regression Model with all the variables helps us to visualize which variables are most realted to number of accidents. \n",
    "By looking at p-values we can observe that the Total Snow is statistically significant. \n",
    "The next two variables that are also seemed to be statistically significant are Wind Gust and Cloud Cover. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newOls = sm.ols(formula=\"accidentCount ~ tempC + visibility + totalSnow_cm + sunHour + DewPointC + WindGustKmph + cloudcover + precipMM\", data=mergedDataClean).fit()\n",
    "newOls.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➡\n",
    "From original regression model we have predicted that the \"Total Snow\", \"Wind Gust\" and \"Cloud Cover\" variables have the most impact on accident prediction.\n",
    "\n",
    "But looking at the next two models we can observe that:\n",
    "\n",
    "Creating a Regression Model with just those 3 variables mentioned above, is not that great and doesn't provide a valuable information, and we can detect that by looking at R-squared (which is low)  and p-values (two high).\n",
    "When we include interaction, which is provided in the second model, we observe that the R-square is higher and p-values are smaller for most variables. \n",
    "With that information we can conclude, that Total Snow is still significant, and the Total Snow with Cloud Cover is also very significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newOls = sm.ols(formula=\"accidentCount ~ totalSnow_cm + WindGustKmph + cloudcover\", data=mergedDataClean).fit()\n",
    "newOls.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newOls = sm.ols(formula=\"accidentCount ~ totalSnow_cm*WindGustKmph*cloudcover\", data=mergedDataClean).fit()\n",
    "newOls.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to see how much of impact does snow have on accident prediction. The next Regression Model we chose to not include the \"Total Snow\" variable. Just by looking at R-square value we can observe, that this is not a great model for predicting accidents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newOls = sm.ols(formula=\"accidentCount ~ WindGustKmph*cloudcover\", data=mergedDataClean).fit()\n",
    "newOls.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saltLakeAccidents.groupby('Date')['CRASH_SEVERITY_ID'].mean()\n",
    "#IDEA: plot combined weather variables and some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Accident Severity\n",
    "Often times, first responders are unaware of what awaits them at the scene of an accident. This can result in being unprepared and potiential adverse outcomes. Being able to predict accident severity based off a variety of features could allow for more appropriate preparation and potentially better outcomes. First responders would have a better idea of what they are geting into which could lead to more efficient resourcing and appropriate staff on hand. <br/>\n",
    "\n",
    "Our analysis will span 3 classification models; Random Forest, K-Nearest Neighbors, and Suport Vector Machines. We will go through the excersize of plugging in all and a selection of predictive features into the models to determine if we can get away with using fewer features to avoid overfitting. Additionally, given the significant imbalence of minor accident classifications to major, we will also perform a binary classification approach to see if we can determine a major vs minor accident based off the same feature set. The aim would be to achieve a high level of accuracy and, in the case of the binary classification, a high AUC score to maintain legetimacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Prep and Function Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display  \n",
    "import pydotplus \n",
    "from scipy import misc\n",
    "\n",
    "allFeatures = [\n",
    " 'WORK_ZONE_RELATED',\n",
    " 'PEDESTRIAN_INVOLVED',\n",
    " 'BICYCLIST_INVOLVED',\n",
    " 'MOTORCYCLE_INVOLVED',\n",
    " 'IMPROPER_RESTRAINT',\n",
    " 'UNRESTRAINED',\n",
    " 'DUI',\n",
    " 'INTERSECTION_RELATED',\n",
    " 'WILD_ANIMAL_RELATED',\n",
    " 'DOMESTIC_ANIMAL_RELATED',\n",
    " 'OVERTURN_ROLLOVER',\n",
    " 'COMMERCIAL_MOTOR_VEH_INVOLVED',\n",
    " 'TEENAGE_DRIVER_INVOLVED',\n",
    " 'OLDER_DRIVER_INVOLVED',\n",
    " 'NIGHT_DARK_CONDITION',\n",
    " 'SINGLE_VEHICLE',\n",
    " 'DISTRACTED_DRIVING',\n",
    " 'DROWSY_DRIVING',\n",
    " 'ROADWAY_DEPARTURE']\n",
    "someFeatures = [\n",
    " 'PEDESTRIAN_INVOLVED',\n",
    "  'BICYCLIST_INVOLVED',\n",
    " 'MOTORCYCLE_INVOLVED',\n",
    " 'NIGHT_DARK_CONDITION',\n",
    " 'TEENAGE_DRIVER_INVOLVED',\n",
    "  'DISTRACTED_DRIVING'\n",
    "]\n",
    "\n",
    "def renderTree(my_tree, features):\n",
    "    # hacky solution of writing to files and reading again\n",
    "    # necessary due to library bugs\n",
    "    filename = \"temp.dot\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f = tree.export_graphviz(my_tree, \n",
    "                                 out_file=f, \n",
    "                                 feature_names=features, \n",
    "                                 class_names=[\"1\", \"2\",\"3\",\"4\",\"5\"],  \n",
    "                                 filled=True, \n",
    "                                 rounded=True,\n",
    "                                 special_characters=True)\n",
    "  \n",
    "    dot_data = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        dot_data = f.read()\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    image_name = \"temp.png\"\n",
    "    graph.write_png(image_name)  \n",
    "    display(Image(filename=image_name))\n",
    "\n",
    "def splitData(features, binary=False):\n",
    "    severityPredictors = saltLakeAccidents[features].values\n",
    "    if binary:\n",
    "        severityLabels = saltLakeAccidents[\"MINOR_ACCIDENT\"].values\n",
    "    else:\n",
    "        severityLabels = saltLakeAccidents[\"CRASH_SEVERITY_ID\"].values\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(severityPredictors, severityLabels, test_size=0.5, random_state=1)\n",
    "    return xTrain, xTest, yTrain, yTest\n",
    "\n",
    "def minSplitData(features, binary=False):\n",
    "    miniSLCAccidents = saltLakeAccidents.sample(10000)\n",
    "    severityPredictors = miniSLCAccidents[features].values\n",
    "    if binary:\n",
    "        severityLabels = miniSLCAccidents[\"MINOR_ACCIDENT\"].values\n",
    "    else:\n",
    "        severityLabels = miniSLCAccidents[\"CRASH_SEVERITY_ID\"].values\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(severityPredictors, severityLabels, test_size=0.5, random_state=1)\n",
    "    return xTrain, xTest, yTrain, yTest\n",
    "\n",
    "def majorSplitData(features):\n",
    "    severityPredictors = saltLakeMajorAccidents[features].values\n",
    "    severityLabels = saltLakeMajorAccidents[\"CRASH_SEVERITY_ID\"].values\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(severityPredictors, severityLabels, test_size=0.5, random_state=1)\n",
    "    return xTrain, xTest, yTrain, yTest\n",
    "\n",
    "def majorMinSplitData(features):\n",
    "    severityPredictors = saltLakeMajorAccidents[features].values\n",
    "    severityLabels = saltLakeMajorAccidents[\"CRASH_SEVERITY_ID\"].values\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(severityPredictors, severityLabels, test_size=0.5, random_state=1)\n",
    "    return xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Analysis (MultiClass)\n",
    "The Random Forest multi class approach with all the variables seems to yield decent accuracy at appx 73% with a max tree depth of 10 levels. However, closer observation of the confusion matrix indicates that it may be a bit deceiving. The model is clearly trying to always predict a class 1 given its significant size. It therefore achieve a higher accuracy by almost always predicting 1. Perhaps a binary approach would yield better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = splitData(allFeatures)\n",
    "randomForest = RandomForestClassifier(bootstrap=True, n_estimators=500, max_depth=10)\n",
    "trainedForest = randomForest.fit(xTrain, yTrain)\n",
    "yPredictTrain = trainedForest.predict(xTrain)\n",
    "yPredict = trainedForest.predict(xTest)\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_true = yTest, y_pred = yPredict))\n",
    "print('Accuracy on training data = ', metrics.accuracy_score(y_true = yTrain, y_pred = yPredictTrain))\n",
    "print('Accuracy on test data = ', metrics.accuracy_score(y_true = yTest, y_pred = yPredict))\n",
    "Cs = np.arange(1,12)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs):\n",
    "    crossTree = RandomForestClassifier(bootstrap=True, n_estimators=500,max_depth=C)\n",
    "    treeCrossModel = crossTree.fit(xTrain, yTrain)\n",
    "    scores = cross_val_score(estimator = treeCrossModel, X = xTest, y = yTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.title('Best Max Depth')\n",
    "plt.show()\n",
    "\n",
    "# renderTree(decisionTree, allFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Analysis (Binary)\n",
    "Splitting the data into 2 categories, minor accidents and major accidents, seems to yield better results. Minor accidents would be defined as an accident with a severity score of 2 or less. Running the model at this threshold yields very high accuracy at 91%. To legitimize the model, we felt it would be appropirate to look at the AUC metric to ensure that the minor accident class size was not inappropriately influencing the results. Looking at the AUC score (0.63) it seems that the class sizes do play a role in the model outcomes. This AUC score is not horrible, but it would indicate that the results are perhaps not entirely reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = splitData(someFeatures, True)\n",
    "randomForest = RandomForestClassifier(bootstrap=True, n_estimators=300, max_depth=7)\n",
    "trainedForest = randomForest.fit(xTrain, yTrain)\n",
    "\n",
    "yPredictTrain = trainedForest.predict(xTrain)\n",
    "yPredict = trainedForest.predict(xTest)\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_true = yTest, y_pred = yPredict))\n",
    "print('Accuracy on training data = ', metrics.accuracy_score(y_true = yTrain, y_pred = yPredictTrain))\n",
    "print('Accuracy on test data = ', metrics.accuracy_score(y_true = yTest, y_pred = yPredict))\n",
    "Cs = np.arange(1,10)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs):\n",
    "    crossTree = RandomForestClassifier(bootstrap=True, n_estimators=300,max_depth=C)\n",
    "    treeCrossModel = crossTree.fit(xTrain, yTrain)\n",
    "    scores = cross_val_score(estimator = treeCrossModel, X = xTest, y = yTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.title('Best Max Depth')\n",
    "plt.show()\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = trainedForest.predict_proba(xTest)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(yTest, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Analysis (MultiClass Major Accidents Only)\n",
    "In an effort to decrease the influence of the larger class sizes (1,2), we tried to narrow the scope to just major accidents with a severity of 3 or above. This yielded an accuracy of 83% which seems promising. However, a look at the confusion matrix seems to indicate that the model is simply always predicting 3 given its size in relation to the other severities. So again not terribly accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = majorSplitData(someFeatures)\n",
    "randomForest = RandomForestClassifier(bootstrap=True, n_estimators=500, max_depth=7)\n",
    "trainedForest = randomForest.fit(xTrain, yTrain)\n",
    "\n",
    "yPredictTrain = trainedForest.predict(xTrain)\n",
    "yPredict = trainedForest.predict(xTest)\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_true = yTest, y_pred = yPredict))\n",
    "print('Accuracy on training data = ', metrics.accuracy_score(y_true = yTrain, y_pred = yPredictTrain))\n",
    "print('Accuracy on test data = ', metrics.accuracy_score(y_true = yTest, y_pred = yPredict))\n",
    "Cs = np.arange(1,10)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs):\n",
    "    crossTree = RandomForestClassifier(bootstrap=True, n_estimators=500,max_depth=C)\n",
    "    treeCrossModel = crossTree.fit(xTrain, yTrain)\n",
    "    scores = cross_val_score(estimator = treeCrossModel, X = xTest, y = yTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.title('Best Max Depth')\n",
    "plt.show()\n",
    "\n",
    "# renderTree(decisionTree, allFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Analysis (MultiClass)\n",
    "Insert Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xKTrain, xKTest, yKTrain, yKTest = splitData(someFeatures)\n",
    "kNewsModel = KNeighborsClassifier(n_neighbors = 9)\n",
    "kNewsModel.fit(xKTrain,yKTrain)\n",
    "y_kNews_pred = kNewsModel.predict(xKTest)\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_true = yKTest, y_pred = y_kNews_pred))\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yKTrain, y_pred = kNewsModel.predict(xKTrain)))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = yKTest, y_pred = y_kNews_pred))\n",
    "Ks = np.arange(1,12)\n",
    "Accuracies = np.zeros(Ks.shape[0])\n",
    "for i,C in enumerate(Ks):\n",
    "    kCrossModel = KNeighborsClassifier(n_neighbors = C)\n",
    "    scores = cross_val_score(estimator = kCrossModel, X = xKTest, y = yKTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Ks,Accuracies)\n",
    "plt.title('Best Value of K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Analysis (Binary)\n",
    "Insert Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xKTrain, xKTest, yKTrain, yKTest = splitData(someFeatures, True)\n",
    "kNewsModel = KNeighborsClassifier(n_neighbors = 3)\n",
    "kNewsModel.fit(xKTrain,yKTrain)\n",
    "y_kNews_pred = kNewsModel.predict(xKTest)\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_true = yKTest, y_pred = y_kNews_pred))\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yKTrain, y_pred = kNewsModel.predict(xKTrain)))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = yKTest, y_pred = y_kNews_pred))\n",
    "Ks = np.arange(1,12)\n",
    "Accuracies = np.zeros(Ks.shape[0])\n",
    "for i,C in enumerate(Ks):\n",
    "    kCrossModel = KNeighborsClassifier(n_neighbors = C)\n",
    "    scores = cross_val_score(estimator = kCrossModel, X = xKTest, y = yKTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Ks,Accuracies)\n",
    "plt.title('Best Value of K')\n",
    "plt.show()\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = kNewsModel.predict_proba(xKTest)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(yKTest, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Analysis (MultiClass Major Accidents Only)\n",
    "Insert Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "xKTrain, xKTest, yKTrain, yKTest = majorSplitData(someFeatures)\n",
    "kNewsModel = KNeighborsClassifier(n_neighbors = 6)\n",
    "kNewsModel.fit(xKTrain,yKTrain)\n",
    "y_kNews_pred = kNewsModel.predict(xKTest)\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_true = yKTest, y_pred = y_kNews_pred))\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yKTrain, y_pred = kNewsModel.predict(xKTrain)))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = yKTest, y_pred = y_kNews_pred))\n",
    "Ks = np.arange(1,12)\n",
    "Accuracies = np.zeros(Ks.shape[0])\n",
    "for i,C in enumerate(Ks):\n",
    "    kCrossModel = KNeighborsClassifier(n_neighbors = C)\n",
    "    scores = cross_val_score(estimator = kCrossModel, X = xKTest, y = yKTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Ks,Accuracies)\n",
    "plt.title('Best Value of K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (MultiClass)\n",
    "Insert Interpretation Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "xTrain, xTest, yTrain, yTest = splitData(someFeatures)\n",
    "svmModel = svm.SVC(kernel='rbf',C=2,gamma='auto')\n",
    "svmModel.fit(xTrain, yTrain)\n",
    "y_svm_pred = svmModel.predict(xTest)\n",
    "print(metrics.confusion_matrix(y_true = yTest, y_pred = y_svm_pred))\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = svmModel.predict(xTrain)))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = yTest, y_pred = y_svm_pred))\n",
    "Cs = np.linspace(.01,5,10)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs):\n",
    "    svmCrossModel = svm.SVC(kernel='rbf', C = C,gamma='auto')\n",
    "    scores = cross_val_score(estimator = svmCrossModel, X = xTest, y = yTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Binary)\n",
    "Insert Interpretation Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = splitData(someFeatures, True)\n",
    "svmModel = svm.SVC(kernel='rbf',C=1,gamma='auto',probability=True)\n",
    "svmModel.fit(xTrain, yTrain)\n",
    "y_svm_pred = svmModel.predict(xTest)\n",
    "print(metrics.confusion_matrix(y_true = yTest, y_pred = y_svm_pred))\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = svmModel.predict(xTrain)))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = yTest, y_pred = y_svm_pred))\n",
    "Cs = np.linspace(.01,5,10)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs):\n",
    "    svmCrossModel = svm.SVC(kernel='rbf', C = C,gamma='auto')\n",
    "    scores = cross_val_score(estimator = svmCrossModel, X = xTest, y = yTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()\n",
    "\n",
    "probs = svmModel.predict_proba(xTest)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(yTest, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (MultiClass Major Accidents Only)\n",
    "Insert Interpretation Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "xTrain, xTest, yTrain, yTest = majorSplitData(someFeatures)\n",
    "svmModel = svm.SVC(kernel='rbf',C=2,gamma='auto')\n",
    "svmModel.fit(xTrain, yTrain)\n",
    "y_svm_pred = svmModel.predict(xTest)\n",
    "print(metrics.confusion_matrix(y_true = yTest, y_pred = y_svm_pred))\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = svmModel.predict(xTrain)))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = yTest, y_pred = y_svm_pred))\n",
    "Cs = np.linspace(.01,500,50)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs):\n",
    "    svmCrossModel = svm.SVC(kernel='rbf', C = C,gamma='auto')\n",
    "    scores = cross_val_score(estimator = svmCrossModel, X = xTest, y = yTest, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
